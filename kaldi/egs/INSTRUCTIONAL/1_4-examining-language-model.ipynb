{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4: Examining language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ARPA` (그리고 `iARPA`) 포맷은 해석하기 매우 쉽습니다. 만약 아직 확인하지 않았다면 [이 곳](https://cmusphinx.github.io/wiki/arpaformat/)에서 내용을 확인하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat resource_files/language_model/animal_lm-2_gram.iarpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `PyNLPl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 `python`에 있는 [`PyNLPl`](http://pynlpl.readthedocs.io/en/latest/) (pronounced \"pineapple\") 패키지를 이용하여서 language model을 확인해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynlpl.lm.lm as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading in `.iARPA` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ARPALanguageModel()`은 **`iARPA`** 포맷의 language model을 불러오는 명령어입니다.\n",
    "\n",
    "**Note:** 이전 노트북에서 우리는 `sed` 명령어를 활용하여 `.iarpa` 포맷의 파일들을 약간 손을 보았습니다. 그 이유는 `1-gram`과 그 `probability` 사이의 공백이 `\\t(탭)`이 아니라 `\\s(\" \")`으로 처리된 곳이 있었기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm = lm.ARPALanguageModel(\n",
    "    filename=\"resource_files/language_model/animal_lm-2_gram.iarpa\",\n",
    "    base_e=False,  # this will keep the log probabilities in `base 10` so that they match up with the original file\n",
    "    debug=True     # this argument will allow you to more easily see how the data is stored in the object\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 우리는 각각의 `n-gram`이 `<tuple>`의 형태로 저장된 것을 알 수 있습니다. 심지어 `1-gram` 또한 `([word],)`로 저장되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking up **existing** `n-gram`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.ngrams`는 language model에 존재하는 모든 `n-gram`의 정보를 담고 있습니다. 다음과 같은 명령어를 통해 `n-gram`의 세부 정보를 확인할 수 있습니다.\n",
    " - the probability ==> `.prob()`\n",
    " - the backoff probability ==> `.backoff()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"dog\",)), bi_gram_lm.ngrams.backoff((\"dog\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 파일의 값과 비교하여 *probability*나 *backoff*가 제대로 읽혔는지 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat resource_files/language_model/animal_lm-2_gram.iarpa | grep -P \"\\tdog\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\", \"dog\")), bi_gram_lm.ngrams.backoff((\"the\", \"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat resource_files/language_model/animal_lm-2_gram.iarpa | grep -P \"the dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "존재하지 않는 `n-gram`의 probability를 확인하려고 하는 경우, `KeyError`가 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bi_gram_lm.ngrams.prob((\"human\", \"ate\"))\n",
    "except Exception as e:\n",
    "    print(\"n-gram {} doesn't exist in language model\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bi_gram_lm.ngrams.prob((\"the\", \"dog\", \"ate\"))\n",
    "except Exception as e:\n",
    "    print(\"n-gram {} doesn't exist in language model\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating new `n-gram` probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KeyError`가 발생하는 경우는 두 가지가 있습니다. \n",
    "\n",
    " 1. `n-gram`의 사이즈는 language model에 포함되어 있지만 **해당 n-gram이 language model에 존재하지 않을 경우**.\n",
    " 2. **해당 n-gram의 사이즈가 language model보다 클 경우**. (*i.e.,* `2-gram` language model을 불러온 상황에서 `3-gram` probability를 찾으려는 경우)\n",
    " \n",
    "두 경우 모두에서 `.score()` 명령어를 사용할 수 있습니다. 에러가 발생한 `n-gram`의 확률을 계산하기 위해서는 해당 `n-gram`을 `<tuple>`의 형태로 명령어에 입력하면 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `n-gram` is **not present** in language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 상황에서는 `backoff` probability를 활용하게 됩니다. `backoff` probability는 해당 `n-gram`이 데이터에 없을 경우의 probability를 계산하기 위해서 측정된 probability입니다. `2-gram` language model에서는 `1-gram`에만 backoff probability가 존재합니다. backoff probability는 *단어 다음*에 나오는 숫자입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_lm-2_gram.iarpa | grep -A15 -E \"1-grams\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 우리가 toy corpus에 존재하지 않는 `\"human ate\"`라는 `n-gram`의 probability를 계산하려고 할 경우, 다음과 같은 방법으로 probability는 계산됩니다.\n",
    "\n",
    "$p(human\\_ate) = p(human) + p(ate|human) = p(human) + p(ate) + bWt(human)$\n",
    "\n",
    "**Note:** 모든 probability가 **log**로 변환되었기 때문에, 확률을 **곱하지 않고 더하게** 됩니다. 그리고 모든 probability가 `음수`이기 때문에, `0`에 **가까울 수록** corpus에서 더 빈번하게 나타난다는 것을 의미합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.score((\"human\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 더해보면 해당 식이 맞아 들어간다는 것을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bi_gram_lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16d6b99f2816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbi_gram_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbi_gram_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<ate>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbi_gram_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bi_gram_lm' is not defined"
     ]
    }
   ],
   "source": [
    "bi_gram_lm.ngrams.prob((\"human\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"<ate>\",)) + \\\n",
    "bi_gram_lm.ngrams.backoff((\"human\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** 만약 `n-gram`을 `<tuple>`로 입력하지 않았다면, `<string>`은 각각의 글자를 이용한 `n-gram`으로 인식됩니다. 각각의 글자는 language model에 존재하지 않기 때문에, 그 결과는 $p(UNK) * len(string)$과 동일하게 될 것입니다. \n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.score(\"human ate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 0\n",
    "for i in \"human ate\":\n",
    "    result += bi_gram_lm.scoreword(i)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `n-gram` is **larger** than language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 `\"the dog ate\"`의 probability를 `2-gram` language model을 이용하려고 구하려고 할 경우, 다음과 같은 방법으로 계산됩니다. \n",
    "\n",
    "$p(the\\_dog\\_ate) = p(the) + p(dog|the) + p(ate|dog)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.score((\"the\", \"dog\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 각각을 계산해 보면 그 결과가 동일함을 알 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"the\", \"dog\")) + \\\n",
    "bi_gram_lm.ngrams.prob((\"dog\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 만약 `n-gram` 중 하나라도 language model에 존재하지 않을 경우, 다른 경우와 마찬가지로 `backoff` probability를 활용하여 계산하게 됩니다. \n",
    "\n",
    "$p(the\\_triceratops\\_ate) = p(the) + p(triceratops|the) + p(ate|triceratops) = p(the) + p(UNK) + bWt(the) + p(ate) + bWt(triceratops)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.score((\"the\", \"triceratops\", \"ate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"<unk>\",)) + \\\n",
    "bi_gram_lm.ngrams.backoff((\"the\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"ate\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pruning` a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pruning`을 통해서 language model을 더욱 간단하게 할 수 있습니다. `n-gram`에서 많은 수의 n-gram이 굉장히 낮은 빈도로 나타납니다. 심지어 **한 번** 등장한 n-gram 또한 language model에 포함됩니다. 그렇기 때문에 `backoff` probability를 활용하면 빈도가 낮은 `n-gram`을 \"accuracy\"를 크게 낮추지 않고도 제거할 수 있습니다. \n",
    "\n",
    "`IRSTLM` 매뉴얼은 `pruning`을 이렇게 설명합니다. \n",
    "\n",
    "```\n",
    "Large LMs files can be pruned in a smart way by means of the command prune-lm that removes n-grams for which resorting to the back-off results in a small loss.\n",
    "```\n",
    "\n",
    "`librispeech` 데이터는 서로 다른 `pruning` threshold를 가진 두 개의 `pruned 3-gram` language model을 제공합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lh raw_data/ | grep 3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pruning`의 결과로 파일 크기가 작아진 것을 확인할 수 있습니다. `2-gram`과 `3-gram`의 갯수를 확인해 보아도 그 차이는 확연합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n5 raw_data/3-gram.arpa\n",
    "echo ...\n",
    "head -n5 raw_data/3-gram.pruned.1e-7.arpa\n",
    "echo ...\n",
    "head -n5 raw_data/3-gram.pruned.3e-7.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's not necessary to `prune` our toy animal language models, it **is** easy to do with `IRSTLM`.\n",
    "\n",
    "toy corpus에 `pruning`을 적용할 필요는 없지만, `IRSTLM`을 이용하면 쉽게 `pruning`을 적용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export IRSTLM=${KALDI_PATH}/tools/irstlm\n",
    "export PATH=${PATH}:${IRSTLM}/bin\n",
    "prune-lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** 각각의 `n-gram`에 서로 다른 threshold를 적용할 수 있습니다. `librispeech` language model에서도 `1-gram`에는 `pruning`이 적용되지 않은 것을 확인할 수 있습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
